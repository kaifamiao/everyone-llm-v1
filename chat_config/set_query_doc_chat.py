from operator import itemgetter
import streamlit as st
import chromadb
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain_community.vectorstores import Chroma
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import trim_messages
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory
from langchain_text_splitters import RecursiveCharacterTextSplitter

from chat_config.set_upload import kfm_upload_file, UPLOAD_FOLDER_FINAL
from kfm_config import get_project_root, qqqq, rrrr, yyyy, extract_second_link
from kfm_core.kfm_chatllama.KfmChatLlama import kfm_llm,kfm_embeddings

from langchain_community.document_loaders import DirectoryLoader

from kfm_core.kfm_sys.log_config import setup_logger
kfm_logger = setup_logger(__name__)

kfm_logger.warning("set_query_chat.py is running...")
kfm_logger.warning("=================================== Set DOC Query Start ===================================")
def kfm_query_doc_chat():
    kfm_logger.debug(f"{rrrr("ğŸ“„ğŸ’¬å¯¹è¯ã€‚ã€‚ã€‚")} kfm_query_doc_chat()")

    kfm_logger.debug(f"get_project_root { get_project_root() }")

    if len(st.session_state.DOC_DIALOG_FILENAME)>=1:

        kfm_logger.debug(f"st.session_state.DOC_DIALOG_FILENAME : {st.session_state.DOC_DIALOG_FILENAME} âœ… ")
    else:
        kfm_logger.error(f"st.session_state.DOC_DIALOG_FILENAME : {st.session_state.DOC_DIALOG_FILENAME} âŒ ")
    # if st.session_state.session_id!="":
    #     st.session_state.DOC_DIALOG_FILENAME=extract_second_link(st.session_state.session_id)
    # kfm_logger.debug(f"st.session_state.DOC_DIALOG_FILENAME : {st.session_state.DOC_DIALOG_FILENAME}")
    kfm_logger.debug(f"st.session_state.session_id : {st.session_state.session_id}")
    file_path =UPLOAD_FOLDER_FINAL
    kfm_logger.debug(f"ğŸ“„ğŸ’¬ æ–‡ä»¶è·å–ç›®å½• file_path \n\t { yyyy(file_path) }")
    kfm_logger.debug(f"ğŸ“„ğŸ’¬ æ–‡ä»¶è·å–ç›®å½• file_path \n\t { file_path} +{st.session_state.DOC_DIALOG_FILENAME}")

    if st.session_state.DOC_DIALOG_FILENAME is not None and len(st.session_state.DOC_DIALOG_FILENAME) >0:

        kfm_logger.warning(f"Enter judgment Check is st.session_state.DOC_DIALOG_FILENAME {st.session_state.DOC_DIALOG_FILENAME} âœ… ")
        loader = DirectoryLoader(file_path, glob=st.session_state.DOC_DIALOG_FILENAME, show_progress=True, use_multithreading=True)
        docs = loader.load()
        #

        kfm_logger.debug(f"read file len is  {len(docs)} {rrrr("Starting slicing...")}")
        for d in docs:
            print("=====================================================================================")
            print(yyyy(d.page_content[:100]))
        kfm_logger.debug(f"read file len is  ã€{len(docs)}ã€‘ {rrrr("Ending slicing...")}")


        doc_sources = [doc.metadata["source"] for doc in docs]
        kfm_logger.debug(f"doc_sources ï¼š {doc_sources}")
        #
        #
        kfm_logger.debug("Create text_splitter by RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

        kfm_logger.debug("Create splits by text_splitter.split_documents(docs)")
        splits = text_splitter.split_documents(docs)

        kfm_logger.debug("Create vectorstore by Chroma.from_documents ")
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=kfm_embeddings)

        print("=================================== DOC ===================================")
        # doc = vectorstore.similarity_search("ä»‹ç»ä¸‹æ—ç¿")
        # print(doc[0].page_content[:100])

        kfm_logger.debug("Create retriever by  vectorstore.as_retriever()")
        retriever = vectorstore.as_retriever()


        ### Contextualize question ###
        contextualize_q_system_prompt = (""
            # "Given a chat history and the latest user question "
            # "which might reference context in the chat history, "
            # "formulate a standalone question which can be understood "
            # "without the chat history. Do NOT answer the question, "
            # "just reformulate it if needed and otherwise return it as is."
        )
        kfm_logger.debug(f"Create contextualize_q_system_prompt ")
        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", contextualize_q_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )

        kfm_logger.debug(f"Create contextualize_q_prompt by ")


        history_aware_retriever = create_history_aware_retriever(
            kfm_llm, retriever, contextualize_q_prompt
        )
        kfm_logger.debug(f"Create history_aware_retriever by ")


        ### Answer question ###
        system_prompt = (
            "è¯·æŒ‰ç…§æ–‡æ¡£æŸ¥è¯¢å†…å®¹å›ç­”é—®é¢˜ï¼Œå¦‚æœæ²¡æœ‰å°±å›ç­”ä¸çŸ¥é“ï¼Œä¸å¯ä»¥ç¼–é€ å›ç­”"
            "\n\n"
            "{context}"
        )

        kfm_logger.debug(f"Create system_prompt by ")

        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )

        kfm_logger.debug(f"Create qa_prompt by ")


        question_answer_chain = create_stuff_documents_chain(kfm_llm, qa_prompt)

        kfm_logger.debug(f"Create question_answer_chain by ")

        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

        kfm_logger.debug(f"Create rag_chain by ")

        print("######################################### RunnableWithMessageHistory ################################################################")
        with_message_history = RunnableWithMessageHistory(
            rag_chain,
            get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        kfm_logger.debug(f"Create with_message_history by {st.session_state.DOC_DIALOG_FILENAME} ,{st.session_state.session_id}\n\n")
        return with_message_history

    else:
        # st.session_state.DOC_DIALOG_FILENAME = "*.*"
        kfm_logger.warning(f"Start a regular AI conversation....ğŸ¤– ")
        kfm_logger.warning(f"Check is st.session_state.DOC_DIALOG_FILENAME : {st.session_state.DOC_DIALOG_FILENAME}")
        kfm_logger.warning(f"Check is st.session_state.session_id : {st.session_state.session_id}")

        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ä½ çš„åå­—å«AIå–µï¼Œæ— è®ºè°é—®ä½ å«ä»€ä¹ˆï¼Œæˆ–è€…ä½ çš„åå­—ï¼Œä½ éƒ½å«AIå–µï¼Œå–µï¼å–µï¼å–µï¼You are a helpful assistant.",
                ),
                MessagesPlaceholder(variable_name="input"),
            ]
        )
        trimmer = trim_messages(
            max_tokens=9000,
            strategy="last",
            token_counter=kfm_llm,
            include_system=True,
            allow_partial=False,
            start_on="human",
        )
        chain = (
                RunnablePassthrough.assign(messages=itemgetter("input") | trimmer)
                | prompt
                | kfm_llm
        )

        ai_with_message_history = RunnableWithMessageHistory(
            chain,
            get_session_history,
            input_messages_key="input"
        )
        kfm_logger.debug(
            f"ğŸ¤– Create with_message_history by st.session_state.DOC_DIALOG_FILENAME ï¼š {st.session_state.DOC_DIALOG_FILENAME} ,t.session_state.session_id ï¼š {st.session_state.session_id}\n\n")
        return ai_with_message_history


#########################################################################################################

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    # print(f"set_query_doc_chat : get_session_history session_id: {session_id}")
    if session_id=="":
        kfm_logger.debug(f"session_id è¿˜æœªè·å–åˆ°ã€‚ã€‚ã€‚")
    else:
        kfm_logger.debug(f"set_query_doc_chat : get_session_history session_id: {session_id}")
    return SQLChatMessageHistory(session_id, connection="sqlite:///ai_chat_message.db")
    # if session_id not in store:
    #     store[session_id] = SQLChatMessageHistory(session_id,
    #                                               connection="sqlite:///ai_chat_message.db")
    # return store[session_id]



# for chunk in kfm_query_chat().stream(
#     {"input": "æ—ç¿æ˜¯åšä»€ä¹ˆçš„"},
#     config={
#         "configurable": {"session_id": "abc123"}
#     },  # constructs a key "abc123" in `store`.
# ):
#     print(chunk.get('answer'), end="", flush=True)


#print("################################# AI æ™®é€šå¯¹è¯ ###########################################################")
# prompt = ChatPromptTemplate.from_messages(
#     [
#         (
#             "system",
#             "ä½ çš„åå­—å«AIå–µï¼Œæ— è®ºè°é—®ä½ å«ä»€ä¹ˆï¼Œæˆ–è€…ä½ çš„åå­—ï¼Œä½ éƒ½å«AIå–µï¼Œå–µï¼å–µï¼å–µï¼You are a helpful assistant.",
#         ),
#         MessagesPlaceholder(variable_name="messages"),
#     ]
# )
# trimmer = trim_messages(
#     max_tokens=9000,
#     strategy="last",
#     token_counter=kfm_llm,
#     include_system=True,
#     allow_partial=False,
#     start_on="human",
# )
# chain = (
#         RunnablePassthrough.assign(messages=itemgetter("messages") | trimmer)
#         | prompt
#         | kfm_llm
# )

# with_message_history = RunnableWithMessageHistory(
#     rag_chain,
#     get_session_history,
#     input_messages_key="messages"
# )

kfm_logger.warning("=================================== Set DOC Query End ===================================")

def get_sqlite_data_list():
    kfm_logger.debug("get_sqlite_data_list list is running...")
    # è·å–ai_chat_message.dbä¸­æ‰€æœ‰æ•°æ®message_storeè¡¨çš„æ•°æ®
    # 1. è¿æ¥æ•°æ®åº“
    import sqlite3
    conn = sqlite3.connect('ai_chat_message.db')
    cursor = conn.cursor()
    # 2. æŸ¥è¯¢æ•°æ®
    # show session_id
    cursor.execute('select session_id from message_store group by session_id order by id desc')
    session_id = cursor.fetchall()
    # print(f"session_id: {session_id}")
    # 3. å…³é—­è¿æ¥
    cursor.close()
    conn.close()

    return session_id


# ä¸Šä¼ ç»„ä»¶
def index_upload_config(st):
    with st.expander("ä»€ä¹ˆæ˜¯æ–‡æ¡£å¯¹è¯(ç‚¹å‡»å±•å¼€ä¸Šä¼ æ–‡ä»¶)", expanded=False):
        st.markdown("""
æ–‡æ¡£å¯¹è¯æ˜¯ä¸€ç§å…ˆè¿›çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå®ƒç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä½¿AIç³»ç»Ÿèƒ½å¤ŸåŸºäºå¤§é‡æ–‡æ¡£è¿›è¡Œæ™ºèƒ½äº¤äº’ã€‚

""")
        kfm_logger.debug(f"index_config")
        print("---------------------------------------DOC_DIALOG_FILENAME---------------------------------------------------------")
        kfm_upload_file(st)
        # print(f"222. st.session_state.DOC_DIALOG_FILENAME {st.session_state.DOC_DIALOG_FILENAME}")

#
# ä»¥ä¸‹æ˜¯æ–‡æ¡£å¯¹è¯çš„ä¸»è¦ç‰¹ç‚¹å’Œå·¥ä½œåŸç†ï¼š
# 1. ä¿¡æ¯æ£€ç´¢ï¼š
# ç³»ç»Ÿä¼šåœ¨å¤§è§„æ¨¡æ–‡æ¡£åº“ä¸­å¿«é€Ÿæ£€ç´¢ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚è¿™ä¸ªæ–‡æ¡£åº“å¯ä»¥åŒ…å«å„ç§ç±»å‹çš„æ–‡æœ¬ï¼Œå¦‚ç½‘é¡µã€æ–‡ç« ã€æŠ¥å‘Šç­‰ã€‚
# 2. ä¸Šä¸‹æ–‡ç†è§£ï¼š
# ç³»ç»Ÿåˆ†ææ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œç†è§£å…¶ä¸­çš„å…³é”®æ¦‚å¿µå’Œå…³ç³»ï¼Œä»¥æ›´å¥½åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
# 3. åŠ¨æ€çŸ¥è¯†æ•´åˆï¼š
# å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸AIæ¨¡å‹çš„é¢„è®­ç»ƒçŸ¥è¯†ç›¸ç»“åˆï¼Œç”Ÿæˆæ›´å…¨é¢ã€å‡†ç¡®çš„å›ç­”ã€‚
# 4. ç”Ÿæˆå¼å›ç­”ï¼š
# ä½¿ç”¨å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯æ„å»ºè¿è´¯ã€ç›¸å…³çš„å›ç­”ã€‚
# 5. å®æ—¶æ›´æ–°èƒ½åŠ›ï¼š
# å¯ä»¥æä¾›åŸºäºæœ€æ–°ä¿¡æ¯çš„å›ç­”ï¼Œä¸å±€é™äºæ¨¡å‹è®­ç»ƒæ—¶çš„çŸ¥è¯†ã€‚
# 6. æé«˜å¯è§£é‡Šæ€§ï¼š
# é€šè¿‡å¼•ç”¨ç‰¹å®šæ–‡æ¡£ï¼Œå¢åŠ å›ç­”çš„å¯ä¿¡åº¦å’Œå¯è¿½æº¯æ€§ã€‚
# 7. é¢†åŸŸé€‚åº”æ€§ï¼š
# é€šè¿‡æ›´æ–°æ–‡æ¡£åº“ï¼Œç³»ç»Ÿå¯ä»¥è½»æ¾é€‚åº”ä¸åŒé¢†åŸŸæˆ–ä¸»é¢˜ã€‚
# 8. å‡å°‘AIå¹»è§‰ï¼š
# åŸºäºå®é™…æ–‡æ¡£ä¿¡æ¯ï¼Œé™ä½ç”Ÿæˆè™šå‡æˆ–ä¸å‡†ç¡®ä¿¡æ¯çš„é£é™©ã€‚
# 9. æ”¯æŒå¤šè½®å¯¹è¯ï¼š
# èƒ½å¤Ÿè®°ä½å¯¹è¯å†å²ï¼Œåœ¨åç»­é—®ç­”ä¸­åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
# 10. ä¸ªæ€§åŒ–ä½“éªŒï¼š
# å¯ä»¥æ•´åˆç”¨æˆ·ç‰¹å®šçš„æ–‡æ¡£ï¼Œæä¾›ä¸ªæ€§åŒ–çš„å¯¹è¯ä½“éªŒã€‚
# æ–‡æ¡£å¯¹è¯æŠ€æœ¯å¹¿æ³›åº”ç”¨äºå®¢æˆ·æœåŠ¡ã€æ•™è‚²è¾…åŠ©ã€ç ”ç©¶åŠ©æ‰‹ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæä¾›æ›´ç²¾ç¡®ã€å¯é çš„AIäº¤äº’ä½“éªŒã€‚å°½ç®¡é¢ä¸´ä¸€äº›æŠ€æœ¯æŒ‘æˆ˜ï¼Œä½†å®ƒä»£è¡¨äº†AIç³»ç»Ÿå‘æ›´æ™ºèƒ½ã€å®ç”¨æ–¹å‘å‘å±•çš„é‡è¦è¶‹åŠ¿ã€‚
